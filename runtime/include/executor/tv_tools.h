#pragma once

#include <glog/logging.h>
#include <torch/script.h>
#include <string>
#include <unordered_map>
#include <vector>
#include "executor/prim_utils.h"

namespace nnrt
{
#define SPLITE_LINE "================================================"

static std::string showTensorInfo(const torch::Tensor& tensor)
{
    std::stringstream ss;
    ss << " Shape:" << tensor.sizes() << " Dim:" << tensor.dim() << " Dtype:" << tensor.dtype()
       << " Numel:" << tensor.numel() << " Device:" << tensor.device();
    return ss.str();
}

static bool checkTensorEqual(const torch::Tensor& tensor1, const torch::Tensor& tensor2, bool print_tensor = false)
{
    DLOG(INFO) << "tensor1:" << showTensorInfo(tensor1);
    DLOG(INFO) << "tensor2:" << showTensorInfo(tensor2);

    DLOG(INFO) << SPLITE_LINE;
    DLOG(INFO) << tensor1;
    DLOG(INFO) << SPLITE_LINE;
    DLOG(INFO) << tensor2;

    bool ret = tensor1.equal(tensor2);
    return ret;
}

class TVComparator
{
   public:
    ~TVComparator() { tv_.clear(); }
    TVComparator(const TVComparator&) = delete;
    TVComparator& operator=(const TVComparator&) = delete;
    static TVComparator& getInstance()
    {
        static TVComparator tv_comparator;
        return tv_comparator;
    }

   private:
    TVComparator() {}

   public:
    /**
     * @brief Load Test vector from binary file, the Tensor of test vector are loaded as default CPUTensor. If loaded,
     * the tensor are saved into a unordered_map. binary file (.bin) it is not pytroch (.pt/.pth) file, since .pt file
     * failed to load in libtorch c++ API, parse .pt file may depend on python pickle module
     *
     * @param bin_file the binary file is `generated by tensor.cpu().numpy().tofile()`
     * @param shape the shape of test_vector
     * @param dtype the data type of test_vector
     * @param ans_key_name set a key/tag name for this test_vector
     */
    void loadTV(const std::string& file, const std::vector<int64_t>& shape, DataType dtype,
                const std::string& ans_key_name)
    {
        auto tensor = loadTensor(file, shape, dtype);
        tv_.insert({ans_key_name, tensor});
    }

    void clear() { tv_.clear(); }

    /**
     * @brief Compare 2 tensors using torch.equal(), return true, only if both size & element value equal
     *
     * @param input_tensor the tensor needed to compare, since the ans_tensor is loaded as CPUTensor, so input_tensor
     * need to be CPUTensor also.
     * @param ans_key_name the key_name of answer, ans_tensor = unordered_map[key_name]
     * @param print_tensor if print the values
     * @return true
     * @return false
     */
    bool compare(const torch::Tensor& input_tensor, const std::string& ans_key_name, bool print_tensor = false)
    {
        auto iter = tv_.find(ans_key_name);
        assert(iter != tv_.end() && "Test vector not exist!");
        auto other_tensor = iter->second;
        bool status = checkTensorEqual(input_tensor, other_tensor, print_tensor);
        if (status) {
            DLOG(INFO) << "TVComparator: Success !";
        } else {
            DLOG(INFO) << "TVComparator: Failed !";
        }
        return status;
    }

   private:
    std::unordered_map<std::string, torch::Tensor> tv_;
};

}  // namespace nnrt
